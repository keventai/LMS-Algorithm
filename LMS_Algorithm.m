clc
clear all
close all

%% Generate some data samples known as signals
num_samples = 100;                    % number of samples
signal_r = randn(1,num_samples);      % generate random signals number from 1 to the total of number of data samples inputted
m_gradient = 5;                       % slope of a line
c = -2;                               % y-intercept

yf = m_gradient*signal_r + c;         % Linear system to be determined
SNR = 20;                             % 20(db) signal to noise ratio
y = awgn(yf,SNR);                     % Noisy output being generated by adding the additive white gaussian noise to the vector of signal yf; assume the power of yf is 0 dBW

%% LMS algorithm parameters being implemented
iterations = 300;                     % number of times each sample is passed to LMS
eta = 2e-4;                           % choose optimal value of learning rate
mm = randn();                         % initial linear model's slope with random numbers
cm = randn();                         % initial linear model's y-intercept with random numbers
ym = mm*signal_r + cm;                % generating output from obtained model

e = y-ym;                             % initial error in modelling
E = mean(e.^2);                       % initial mean squared error (MSE)

%% Training the initial model to keep track and store the output values of the linear system mentioned above

for i = 1:iterations
    mm = mm + 2*eta*signal_r*e';      % estimated linear model's slope with training numbers
    cm = cm + 2*eta*sum(e);           % estimated linear model's y-intercept with training numbers
    ym = mm*signal_r + cm;            % regenerating output from obtained model
    e = y-ym;                         % error in modelling
    E = [E mean(e.^2)];               % mean squared error (MSE)
end

plot(10*log10(E))                     % plot the mean squared error graph
grid minor                            % toggles the visibility of minor grid lines; minor grid lines lie between the tick marks.
xlabel('Number of Iterations')
ylabel('Mean squared errors (MSE)')
title('LMS Algorithm')

[m_gradient c;mm cm]; % comparision of desired parameters and obtained parameters
display('Note that the cost function is reduced to the noise level of the system')